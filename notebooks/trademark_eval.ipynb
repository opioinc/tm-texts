{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load 500 examples of EURIPO trademark court decisions\n",
    "\n",
    "* Columns: 'mark', 'earlier_mark', 'target_label'\n",
    "* 250 examples of NLOC (target_label == 0), 250 examples of LOC (target_label == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mark</th>\n",
       "      <th>earlier_mark</th>\n",
       "      <th>target_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2373</td>\n",
       "      <td>LOOP</td>\n",
       "      <td>JOOP!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5793</td>\n",
       "      <td>ABACELL</td>\n",
       "      <td>AVICEL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>SUVIQUE</td>\n",
       "      <td>ZUBIQUE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1272</td>\n",
       "      <td>EVELYN</td>\n",
       "      <td>EBELIN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>EBELIN</td>\n",
       "      <td>EVELYN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mark earlier_mark  target_label\n",
       "2373     LOOP        JOOP!             1\n",
       "5793  ABACELL       AVICEL             1\n",
       "294   SUVIQUE      ZUBIQUE             1\n",
       "1272   EVELYN       EBELIN             1\n",
       "1990   EBELIN       EVELYN             1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_set = pd.read_csv('../500_tm_loc_decisions.csv', index_col=0)\n",
    "data_set.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model builder class\n",
    "\n",
    "* Instantiate with a data source, a features dict (key = name, value = transform function) and an instantiated classifer algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "class ModelBuilder:\n",
    "    \n",
    "    def __init__(self, data_source, features, algorithm):\n",
    "        self.data_source = data_source\n",
    "        self.features = features\n",
    "        self.algorithm = algorithm\n",
    "        self.metrics = {}\n",
    "        self.false_positives = []\n",
    "        self.false_negatives = []        \n",
    "        \n",
    "    def predict(self, mark, earlier_mark):\n",
    "        p_vals = []\n",
    "        for feature_key, feature_func in self.features.items():\n",
    "            p_vals.append(feature_func(mark, earlier_mark))\n",
    "        \n",
    "        frame = pd.DataFrame([p_vals], columns=self.features.keys())\n",
    "        selected = frame.loc[:, ]\n",
    "        prediction = self.pipeline.predict(selected)[0]\n",
    "        predicted_probs = self.pipeline.predict_proba(selected)[0]\n",
    "        \n",
    "        print(\"Prediction: %d (%s) probability = NLOC=%.3f, LOC=%.3f\" % (prediction, \n",
    "                                                                         'LOC' if prediction else 'NLOC', \n",
    "                                                                         predicted_probs[0],\n",
    "                                                                         predicted_probs[1]))\n",
    "    def build(self):\n",
    "        # Build the model features ...\n",
    "        for feature_key, feature_func in self.features.items():\n",
    "            print(\"Generating %s ... \" % feature_key)\n",
    "            for _index, row in self.data_source.iterrows():\n",
    "                self.data_source.at[_index, feature_key] = feature_func(row['mark'], row['earlier_mark'])\n",
    "            \n",
    "        X = self.data_source[self.features.keys()]\n",
    "        y = self.data_source['target_label']\n",
    "        \n",
    "        train_dataset, test_dataset, train_labels, test_labels = train_test_split(\n",
    "            X, y, test_size=0.25, random_state=0)\n",
    "        \n",
    "        self.pipeline = Pipeline([\n",
    "            ('scaler', preprocessing.MinMaxScaler()),\n",
    "            ('clf', self.algorithm)\n",
    "        ])\n",
    "        \n",
    "        self.pipeline.fit(train_dataset, train_labels)\n",
    "        \n",
    "        y_score = self.pipeline.predict(test_dataset)\n",
    "\n",
    "        cm = confusion_matrix(test_labels, y_score, labels=[1,0])\n",
    "        \n",
    "        cm_dict = {\n",
    "            'TP': int(cm[0][0]),\n",
    "            'FP': int(cm[0][1]),\n",
    "            'FN': int(cm[1][0]),\n",
    "            'TN': int(cm[1][1])\n",
    "        }\n",
    "        \n",
    "        err_metrics_pretty = (\n",
    "            'A        Predicted       Error Metrics\\n' \n",
    "            'c       True False\\n'\n",
    "            't  True  %3d   %3d     F1 Score %6.3f\\n' \n",
    "            'u                     Precision %6.3f\\n'\n",
    "            'a False  %3d   %3d       Recall %6.3f\\n'\n",
    "            'l                       ROC AUC %6.3f') % (int(cm[0][0]), \n",
    "                                                        int(cm[0][1]), \n",
    "                                                        f1_score(test_labels, y_score),\n",
    "                                                        precision_score(test_labels, y_score),\n",
    "                                                        int(cm[1][0]), \n",
    "                                                        int(cm[1][1]),\n",
    "                                                        recall_score(test_labels, y_score),\n",
    "                                                        roc_auc_score(test_labels, y_score))\n",
    " \n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(test_labels, y_score)\n",
    "                \n",
    "        cnt = None\n",
    "        for cnt, _idx in enumerate(test_dataset.index[(np.ravel(test_labels.values) == 1.0) & (y_score == 0.0)]):\n",
    "            row = self.data_source.loc[_idx]\n",
    "            self.false_positives.append(\" FP : %s vs %s\" % (row['mark'], row['earlier_mark']))\n",
    "        \n",
    "        cnt = None\n",
    "        for cnt, _idx in enumerate(test_dataset.index[(np.ravel(test_labels.values) == 0.0) & (y_score == 1.0)]):\n",
    "            row = self.data_source.loc[_idx]\n",
    "            self.false_negatives.append(\" FN : %s vs %s\" % (row['mark'], row['earlier_mark']))\n",
    "            \n",
    "        print()\n",
    "        print(err_metrics_pretty)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Class\n",
    "\n",
    "If you want to add new features then define them here as methods and wire them up in the `feature_gen` map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jellyfish\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "class TrademarkFeatures:\n",
    "\n",
    "    def __init__(self, mark, earlier_mark):\n",
    "        self.mark = mark\n",
    "        self.earlier_mark = earlier_mark\n",
    "\n",
    "    def levenshtein_distance(self):\n",
    "        return jellyfish.levenshtein_distance(self.mark.lower(), self.earlier_mark.lower())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the feature set\n",
    "\n",
    "- TODO: add other feature suggestions ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "feature_gen = {\n",
    "    'ALWAYS_0': lambda m1,m2: 0\n",
    "    #'CASE_INSENSITIVE_MATCH': lambda mark, earlier_mark: mark.lower() == earlier_mark.lower(),\n",
    "    #'LEVENSHTEIN_DISTANCE': lambda mark, earlier_mark: TrademarkFeatures(mark, earlier_mark).levenshtein_distance() \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute!\n",
    "\n",
    "- Returns error metrics (f1_score, precision, recall, confusion matrix)\n",
    "- Example of false positives + true negatives\n",
    "- Use these as a feedback loop to come up with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A - Candidate Model\n",
      "---------------------\n",
      "Generating ALWAYS_0 ... \n",
      "\n",
      "A        Predicted       Error Metrics\n",
      "c       True False\n",
      "t  True   62     0     F1 Score  0.663\n",
      "u                     Precision  0.496\n",
      "a False   63     0       Recall  1.000\n",
      "l                       ROC AUC  0.500\n"
     ]
    }
   ],
   "source": [
    "print(\"Model A - Candidate Model\")\n",
    "print(\"---------------------\")\n",
    "model_A = ModelBuilder(data_set, \n",
    "             feature_gen, \n",
    "             LogisticRegression(solver='lbfgs'),\n",
    "             #KNeighborsClassifier(),\n",
    "            ).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model B\n",
      "---------------------\n",
      "Generating ALWAYS_0 ... \n",
      "\n",
      "A        Predicted       Error Metrics\n",
      "c       True False\n",
      "t  True   62     0     F1 Score  0.663\n",
      "u                     Precision  0.496\n",
      "a False   63     0       Recall  1.000\n",
      "l                       ROC AUC  0.500\n",
      "\n",
      "False Positive errors (our model predicted NLOC but was actually LOC):\n",
      "\n",
      "\n",
      "False Negative errors (our model predicted LOC but was actually NLOC):\n",
      " FN : ROADFLIRT vs FlirtCast\n",
      " FN : shoo-be-doo vs SEA-DOO\n",
      " FN : RELEASE THE BEAST vs REHAB THE BEAST!\n",
      " FN : WINE PASSION vs WINES OF PASSION\n",
      " FN : ARCOX vs ARCAL\n",
      " FN : i.am vs IM+\n",
      " FN : RUBINO ROSSO vs RIBENA\n",
      " FN : Adero vs ATEGO\n",
      " FN : Xplus vs CYPLUS\n",
      " FN : IVINCI vs DA VINCI\n",
      " FN : SEVA vs SERA\n",
      " FN : STRESSEN vs STRESSGEN\n",
      " FN : LACTOFIDUS vs LACTOFIL ULTRALIFE\n",
      " FN : SUNLINE vs SUN MICROSYSTEMS FINANCE\n",
      " FN : Stonecare vs STONCOR\n",
      " FN : RELEASE THE BEAST vs UNLEASH THE NITRO BEAST!\n",
      " FN : SISVEL WE PROTECT IDEAS vs IDEAS TV\n",
      " FN : PELI PROGEAR vs PELÉ\n",
      " FN : BROOKS BROTHERS vs Croops\n",
      " FN : blossoms of liberty vs LIBERTY OF LONDON\n",
      " FN : IMPLANTEO vs IMPLANTMED\n",
      " FN : RAP vs RAMP\n",
      " FN : ATOVIT  A Z vs ALCOVIT\n",
      " FN : PASTA KING vs FANCY KING\n",
      " FN : RUGBY TOUR OPTIQUE vs TOTAL RUGBY\n",
      " FN : SUNLINE vs SUNSERVICE\n",
      " FN : EASYWALL vs EASYGYM\n",
      " FN : WHERE IMAGINATION BEGINS vs IMAGINARIUM\n",
      " FN : CLASH-A-RAMA vs Skyrama\n",
      " FN : HOT CLICK vs HORLICKS\n",
      " FN : Neo Classic vs NEOLOC\n",
      " FN : VITA vs ITA DENT\n",
      " FN : BRONZE ELEGANCE vs SEA BRONZE\n",
      " FN : Calcigloss vs CALCIPORE\n",
      " FN : SILOSIN vs FILOSIR\n",
      " FN : NIGHT NECTAR vs AGUARDIENTE NECTAR AZUL\n",
      " FN : Luna illuminazione vs LUNOO\n",
      " FN : EASY MÖBEL vs EASYWORKROOMS\n",
      " FN : IBERSKI vs EVERSKI\n",
      " FN : SISVEL WE PROTECT IDEAS vs IDEAS TELCEL DIRECTO\n",
      " FN : RAIDER vs REBER\n",
      " FN : PAMPERS vs CO PAMPA\n",
      " FN : Trader Joe vs Tio Joe\n",
      " FN : Jopee vs JOOP!\n",
      " FN : PJS PARAJUMPERS vs PAYPER JEANS PJ\n",
      " FN : OFFICITY vs OFI SINGLE SELECT\n",
      " FN : EMOVIS vs EMO EUROPEAN MUNICIPALITY OUTSOURCING\n",
      " FN : FERTIUM DURABION vs TRIABON\n",
      " FN : BEE ON vs FEMIBION\n",
      " FN : brokermart vs S.L.\n",
      " FN : screen9 vs NEUF\n",
      " FN : KEBOMED vs PEZOMED\n",
      " FN : QUINTA DOM VICENTE vs VICENTIO\n",
      " FN : BeastPink vs UNLEASH THE CAFFEINE FREE BEAST!\n",
      " FN : VIPS vs VIP TELECOM\n",
      " FN : BALOO vs KALOO\n",
      " FN : D ! SCAN vs BTI SCAN\n",
      " FN : UEFA CHAMPIONS LEAGUE vs CHAMPION\n",
      " FN : WOND'S vs wind\n",
      " FN : Kymera vs KUMERA\n",
      " FN : EASY MÖBEL vs EASYOFFICE\n",
      " FN : BARILLA ARMONIE vs HARMONIA\n",
      " FN : SUNLINE vs SUN STOREDGE\n"
     ]
    }
   ],
   "source": [
    "print(\"Model B\")\n",
    "print(\"---------------------\")\n",
    "model_B = ModelBuilder(data_set, \n",
    "             feature_gen, \n",
    "             LogisticRegression(solver='lbfgs'),\n",
    "             #AdaBoostClassifier(),\n",
    "            ).build()\n",
    "print(\"\\nFalse Positive errors (our model predicted NLOC but was actually LOC):\")\n",
    "print(\"\\n\".join(model_B.false_positives))\n",
    "print(\"\\nFalse Negative errors (our model predicted LOC but was actually NLOC):\")\n",
    "print(\"\\n\".join(model_B.false_negatives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a prediction\n",
    "\n",
    "So you're feeling lucky huh? Let's see what happens when we try to predict the outcome of a few trademarks we haven't seen in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1 (LOC) probability = NLOC=0.499, LOC=0.501\n"
     ]
    }
   ],
   "source": [
    "model_A.predict(\"TOBFIN\", \"TAPFINN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1 (LOC) probability = NLOC=0.499, LOC=0.501\n"
     ]
    }
   ],
   "source": [
    "model_A.predict(\"QUINTA DOM VICENTE\", \"HERO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1 (LOC) probability = NLOC=0.499, LOC=0.501\n"
     ]
    }
   ],
   "source": [
    "model_A.predict(\"RED DRAGON\", \"GREEN DRAGON\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tm-text Kernel",
   "language": "python",
   "name": "tm-text-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
